{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Connect 4 Reinforcement Learning Model"
      ],
      "metadata": {
        "id": "ku6htp7yTCSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "FNAFtJgSTL5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.signal import convolve2d\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "V5O8H31KTBSk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment\n",
        "RED = 1\n",
        "YELLOW = -1\n",
        "EMPTY = 0\n",
        "\n",
        "class C4Env:\n",
        "    def __init__(self, rows=6, cols=7): # 6 rows, 7 cols; stored from 5-0: 0 is top row\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.shape = (rows, cols)\n",
        "        self.board = np.zeros(self.shape, dtype=int)\n",
        "        self.current_player = RED # player 1 and player -1\n",
        "\n",
        "    def render(self, newline=True):\n",
        "        display = '\\n'.join([''.join(['R' if item==RED else 'Y' if item==YELLOW else '.' for item in row]) for row in self.board])\n",
        "        if newline: display += '\\n'\n",
        "        print(display)\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros(self.shape, dtype=int)\n",
        "        self.current_player = RED\n",
        "        return self.board\n",
        "\n",
        "    def legal_actions(self):\n",
        "        return np.nonzero(self.board[0] == 0)[0] # which columns have empty top\n",
        "\n",
        "    def step(self, action):\n",
        "        assert action in self.legal_actions(), \"Invalid action\"\n",
        "        # Find the first empty spot in the column\n",
        "        for row in range(self.rows-1, -1, -1):\n",
        "            if self.board[row, action] == EMPTY:\n",
        "                self.board[row, action] = self.current_player\n",
        "                break\n",
        "        # Check for a win\n",
        "        if self.winning_move(self.current_player):\n",
        "            return self.board, self.current_player, True\n",
        "        # Switch player\n",
        "        self.current_player *= -1\n",
        "        return self.board, None, False\n",
        "\n",
        "    def winning_move(self, piece):\n",
        "        # Define the kernel to use for convolution\n",
        "        kernel = np.array([[1, 1, 1, 1]])\n",
        "\n",
        "        # Check horizontal\n",
        "        if (convolve2d(self.board == piece, kernel, mode='valid') == 4).any():\n",
        "            return True\n",
        "\n",
        "        # Check vertical\n",
        "        if (convolve2d(self.board == piece, kernel.T, mode='valid') == 4).any():\n",
        "            return True\n",
        "\n",
        "        # Check positively sloped diagonals\n",
        "        if (convolve2d(self.board == piece, np.eye(4), mode='valid') == 4).any():\n",
        "            return True\n",
        "\n",
        "        # Check negatively sloped diagonals\n",
        "        if (convolve2d(self.board == piece, np.fliplr(np.eye(4)), mode='valid') == 4).any():\n",
        "            return True\n",
        "\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "eqCPEapQT8xE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, rows=6, cols=7):\n",
        "        super(QNet, self).__init__()\n",
        "        self.a = 64\n",
        "        self.b = 64\n",
        "\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "\n",
        "        self.fc1 = nn.Linear(self.rows * self.cols, self.a)\n",
        "        self.fc2 = nn.Linear(self.a, self.b)\n",
        "        self.fc3 = nn.Linear(self.b, self.cols)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Agent\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, epsilon=0.5, ep_decay=0.9, gamma=0.9, learning_rate=0.1):\n",
        "        self.p_network = QNet()\n",
        "        self.t_network = QNet()\n",
        "        self.optimizer = optim.Adam(self.p_network.parameters(), lr=learning_rate)\n",
        "        self.epsilon = epsilon\n",
        "        self.ep_decay = ep_decay\n",
        "        self.gamma = gamma\n",
        "        self.step = 0\n",
        "\n",
        "    def get_action(self, state, valid_actions, boardEnv=None):\n",
        "        if random.random() > self.epsilon:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
        "                q_values = self.p_network(state_tensor)\n",
        "                valid_q_values = q_values[0][valid_actions]\n",
        "                return valid_actions[torch.argmax(valid_q_values).item()]\n",
        "        else:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
        "        next_state_tensor = torch.FloatTensor(next_state.flatten()).unsqueeze(0)\n",
        "        action_tensor = torch.LongTensor([action])\n",
        "        reward_tensor = torch.FloatTensor([reward])\n",
        "\n",
        "        q_values = self.p_network(state_tensor)\n",
        "        next_q_values = self.t_network(next_state_tensor)\n",
        "\n",
        "        q_value = q_values.gather(1, action_tensor.unsqueeze(1))\n",
        "        next_q_value = next_q_values.max(1)[0].unsqueeze(1)\n",
        "        expected_q_value = reward_tensor + (1 - done) * self.gamma * next_q_value\n",
        "\n",
        "        loss = nn.MSELoss()(q_value, expected_q_value.detach())\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_tnet(self):\n",
        "        self.t_network.load_state_dict(self.p_network.state_dict())\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save({\n",
        "            'model_state_dict': self.p_network.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'epsilon': self.epsilon,\n",
        "            'gamma': self.gamma\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        if os.path.exists(path):\n",
        "            checkpoint = torch.load(path)\n",
        "            self.p_network.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.epsilon = checkpoint['epsilon']\n",
        "            self.gamma = checkpoint['gamma']\n",
        "        else:\n",
        "            print(f\"No saved model found at {path}\")\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        self.epsilon = max(self.ep_min, self.epsilon * self.ep_decay)\n"
      ],
      "metadata": {
        "id": "bLfetBb3lhco"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "eArswsYwTXmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(num_episodes, epsilon=0.5, ep_decay=0.9, gamma=0.01, learning_rate=0.1):\n",
        "    agent1 = QLearningAgent(epsilon=epsilon, ep_decay=ep_decay, gamma=gamma, learning_rate=learning_rate)\n",
        "    agent2 = QLearningAgent(epsilon=epsilon, ep_decay=ep_decay, gamma=gamma, learning_rate=learning_rate)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        boardEnv = C4Env()\n",
        "        board = boardEnv.board\n",
        "        game_over = False\n",
        "        turn = 0\n",
        "        total_turns = 0\n",
        "\n",
        "        while not game_over:\n",
        "            # Get Action\n",
        "            valid_actions = boardEnv.legal_actions()\n",
        "\n",
        "            if turn == 0:\n",
        "                action = agent1.get_action(board, valid_actions, boardEnv)\n",
        "                piece = RED\n",
        "            else:\n",
        "                action = agent2.get_action(board, valid_actions, boardEnv)\n",
        "                piece = YELLOW\n",
        "\n",
        "            boardEnv.step(action)\n",
        "\n",
        "            # Get Rewards\n",
        "            reward = 0\n",
        "            if boardEnv.winning_move(piece):\n",
        "                reward = 1 if turn == 0 else -1\n",
        "                game_over = True\n",
        "            elif np.all(boardEnv.legal_actions == False):\n",
        "                game_over = True\n",
        "\n",
        "            next_state = board.copy()\n",
        "\n",
        "            if turn == 0:\n",
        "                agent1.update(board, action, reward, next_state, game_over)\n",
        "            else:\n",
        "                agent2.update(board, action, -reward, next_state, game_over)\n",
        "\n",
        "            board = next_state\n",
        "            turn = 1 - turn\n",
        "            total_turns += 1\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode {episode} completed: turns - {total_turns}\")\n",
        "            boardEnv.render()\n",
        "\n",
        "\n",
        "    return agent1, agent2\n",
        "\n",
        "# Train the agents\n",
        "agent1, agent2 = train(1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_N7pjVGmNET",
        "outputId": "d27b987e-da07-46f3-ccb8-79b7a21e7f33"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 completed: turns - 20\n",
            ".......\n",
            ".......\n",
            "R....Y.\n",
            "RY..YR.\n",
            "RRRYYYY\n",
            "YRYYRRR\n",
            "\n",
            "Episode 100 completed: turns - 11\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "..Y....\n",
            "..R..RY\n",
            "YYYRRRR\n",
            "\n",
            "Episode 200 completed: turns - 14\n",
            ".......\n",
            "Y......\n",
            "YR.....\n",
            "YR.....\n",
            "YY...R.\n",
            "RRY.RYR\n",
            "\n",
            "Episode 300 completed: turns - 34\n",
            "R..R.YY\n",
            "R..YYYY\n",
            "Y..YRYY\n",
            "RR.YRRR\n",
            "RRYRYYR\n",
            "RRYYRYR\n",
            "\n",
            "Episode 400 completed: turns - 11\n",
            ".......\n",
            ".......\n",
            ".....R.\n",
            "..Y..R.\n",
            "..Y..RR\n",
            ".YY.YRR\n",
            "\n",
            "Episode 500 completed: turns - 15\n",
            ".......\n",
            "...Y...\n",
            "..RR...\n",
            "..RYR..\n",
            "..YYYR.\n",
            "..RYRYR\n",
            "\n",
            "Episode 600 completed: turns - 11\n",
            ".......\n",
            ".......\n",
            "R......\n",
            "RY.....\n",
            "RY.Y...\n",
            "RYYRR..\n",
            "\n",
            "Episode 700 completed: turns - 23\n",
            "....R..\n",
            "....Y..\n",
            ".Y.RR..\n",
            ".RYRR..\n",
            "YRYYYRY\n",
            "YRYRRYR\n",
            "\n",
            "Episode 800 completed: turns - 15\n",
            ".......\n",
            ".R.....\n",
            ".R.....\n",
            ".RR....\n",
            ".RYR.YY\n",
            ".YYRYYR\n",
            "\n",
            "Episode 900 completed: turns - 11\n",
            ".......\n",
            ".......\n",
            "...R...\n",
            "...R...\n",
            "Y..R..R\n",
            "Y.YRYRY\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "-x0gNdtoTfwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to play a game using the trained model\n",
        "def play_game(agent1, agent2):\n",
        "    boardEnv = C4Env()\n",
        "    board = boardEnv.board\n",
        "    game_over = False\n",
        "    turn = 0\n",
        "\n",
        "    while not game_over:\n",
        "        valid_actions = boardEnv.legal_actions()\n",
        "\n",
        "        if turn == 0:\n",
        "            action = agent1.get_action(board, valid_actions)\n",
        "            piece = RED\n",
        "        else:\n",
        "            action = agent2.get_action(board, valid_actions)\n",
        "            piece = YELLOW\n",
        "\n",
        "        boardEnv.step(action)\n",
        "\n",
        "        if boardEnv.winning_move(piece):\n",
        "            boardEnv.render()\n",
        "            print(f\"Player {turn + 1} wins!\")\n",
        "            game_over = True\n",
        "        elif np.all(boardEnv.legal_actions == False):\n",
        "            boardEnv.render()\n",
        "            print(\"It's a draw!\")\n",
        "            game_over = True\n",
        "\n",
        "        boardEnv.render()\n",
        "        turn = 1 - turn\n",
        "\n",
        "# Play a game with the trained agents\n",
        "play_game(agent1, agent2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4YHrDL3r73j",
        "outputId": "f37f7858-c2a3-4795-c160-750a98e2d7dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "..R....\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "..R..Y.\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "..R....\n",
            "..R..Y.\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "..R....\n",
            "..R..YY\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "..R....\n",
            "R.R..YY\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "Y.R....\n",
            "R.R..YY\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "Y.R....\n",
            "R.RR.YY\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "Y......\n",
            "Y.R....\n",
            "R.RR.YY\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "Y.R....\n",
            "Y.R....\n",
            "R.RR.YY\n",
            "\n",
            ".......\n",
            ".......\n",
            "Y......\n",
            "Y.R....\n",
            "Y.R....\n",
            "R.RR.YY\n",
            "\n",
            ".......\n",
            ".......\n",
            "Y......\n",
            "Y.R....\n",
            "Y.R..R.\n",
            "R.RR.YY\n",
            "\n",
            ".......\n",
            ".......\n",
            "Y......\n",
            "Y.R....\n",
            "Y.R..R.\n",
            "R.RRYYY\n",
            "\n",
            ".......\n",
            ".......\n",
            "Y......\n",
            "Y.R....\n",
            "Y.R..RR\n",
            "R.RRYYY\n",
            "\n",
            ".......\n",
            "Y......\n",
            "Y......\n",
            "Y.R....\n",
            "Y.R..RR\n",
            "R.RRYYY\n",
            "\n",
            "Player 2 wins!\n",
            ".......\n",
            "Y......\n",
            "Y......\n",
            "Y.R....\n",
            "Y.R..RR\n",
            "R.RRYYY\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Player v Computer"
      ],
      "metadata": {
        "id": "M7rnpvctTkKU"
      }
    }
  ]
}