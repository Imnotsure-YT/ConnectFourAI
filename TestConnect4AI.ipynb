{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imnotsure-YT/ConnectFourAI/blob/main/TestConnect4AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Connect 4 Reinforcement Learning Model"
      ],
      "metadata": {
        "id": "ku6htp7yTCSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "FNAFtJgSTL5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.signal import convolve2d\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "V5O8H31KTBSk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment\n",
        "RED = 1\n",
        "YELLOW = -1\n",
        "EMPTY = 0\n",
        "\n",
        "class C4Env:\n",
        "    def __init__(self, rows=6, cols=7): # 6 rows, 7 cols; stored from 5-0: 0 is top row\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.shape = (rows, cols)\n",
        "        self.board = np.zeros(self.shape, dtype=int)\n",
        "        self.current_player = RED # player 1 and player -1\n",
        "\n",
        "    def render(self, newline=True):\n",
        "        display = '\\n'.join([''.join(['R' if item==RED else 'Y' if item==YELLOW else '.' for item in row]) for row in self.board])\n",
        "        if newline: display += '\\n'\n",
        "        print(display)\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros(self.shape, dtype=int)\n",
        "        self.current_player = RED\n",
        "        return self.board\n",
        "\n",
        "    def legal_actions(self):\n",
        "        return np.nonzero(self.board[0] == 0)[0] # which columns have empty top\n",
        "\n",
        "    def step(self, action):\n",
        "        assert action in self.legal_actions(), \"Invalid action\"\n",
        "        # Find the first empty spot in the column\n",
        "        for row in range(self.rows-1, -1, -1):\n",
        "            if self.board[row, action] == EMPTY:\n",
        "                self.board[row, action] = self.current_player\n",
        "                break\n",
        "        # Check for a win\n",
        "        if self.winning_move(self.current_player):\n",
        "            return self.board, self.current_player, True\n",
        "        # Switch player\n",
        "        self.current_player *= -1\n",
        "        return self.board, None, False\n",
        "\n",
        "    def winning_move(self, piece):\n",
        "        # Define the kernel to use for convolution\n",
        "        kernel = np.array([[1, 1, 1, 1]])\n",
        "\n",
        "        # Check horizontal\n",
        "        if (convolve2d(self.board == piece, kernel, mode='valid') == 4).any():\n",
        "            return True\n",
        "\n",
        "        # Check vertical\n",
        "        if (convolve2d(self.board == piece, kernel.T, mode='valid') == 4).any():\n",
        "            return True\n",
        "\n",
        "        # Check positively sloped diagonals\n",
        "        if (convolve2d(self.board == piece, np.eye(4), mode='valid') == 4).any():\n",
        "            return True\n",
        "\n",
        "        # Check negatively sloped diagonals\n",
        "        if (convolve2d(self.board == piece, np.fliplr(np.eye(4)), mode='valid') == 4).any():\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n"
      ],
      "metadata": {
        "id": "eqCPEapQT8xE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Hyperparameters\n",
        "\n",
        "num_episodes = 10000\n",
        "epsilon = 0.5\n",
        "ep_decay = 0.9\n",
        "gamma = 0.9\n",
        "learning_rate = 0.1\n",
        "\n",
        "# save path and other misc variables\n",
        "save_path = \"/content/drive/MyDrive/COSMOS 2024/Connect Four/\"\n",
        "\n",
        "# Model parameters\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, rows=6, cols=7):\n",
        "        super(QNet, self).__init__()\n",
        "        self.a = 64\n",
        "        self.b = 64\n",
        "\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "\n",
        "        self.fc1 = nn.Linear(self.rows * self.cols, self.a)\n",
        "        self.fc2 = nn.Linear(self.a, self.b)\n",
        "        self.fc3 = nn.Linear(self.b, self.cols)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Agent\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, epsilon, ep_decay, gamma, learning_rate):\n",
        "        self.p_network = QNet()\n",
        "        self.t_network = QNet()\n",
        "        self.optimizer = optim.Adam(self.p_network.parameters(), lr=learning_rate)\n",
        "        self.epsilon = epsilon\n",
        "        self.ep_decay = ep_decay\n",
        "        self.gamma = gamma\n",
        "        self.step = 0\n",
        "\n",
        "    def get_action(self, state, valid_actions, boardEnv=None):\n",
        "        if valid_actions.size == 0:\n",
        "            return -1\n",
        "        if random.random() > self.epsilon:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
        "                q_values = self.p_network(state_tensor)\n",
        "                valid_q_values = q_values[0][valid_actions]\n",
        "                return valid_actions[torch.argmax(valid_q_values).item()]\n",
        "        else:\n",
        "            return random.choice(valid_actions)\n",
        "\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        state_tensor = torch.FloatTensor(state.flatten()).unsqueeze(0)\n",
        "        next_state_tensor = torch.FloatTensor(next_state.flatten()).unsqueeze(0)\n",
        "        action_tensor = torch.LongTensor([action])\n",
        "        reward_tensor = torch.FloatTensor([reward])\n",
        "\n",
        "        q_values = self.p_network(state_tensor)\n",
        "        next_q_values = self.t_network(next_state_tensor)\n",
        "\n",
        "        q_value = q_values.gather(1, action_tensor.unsqueeze(1))\n",
        "        next_q_value = next_q_values.max(1)[0].unsqueeze(1)\n",
        "        expected_q_value = reward_tensor + (1 - done) * self.gamma * next_q_value\n",
        "\n",
        "        loss = nn.MSELoss()(q_value, expected_q_value.detach())\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_tnet(self):\n",
        "        self.t_network.load_state_dict(self.p_network.state_dict())\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save({\n",
        "            'model_state_dict': self.p_network.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'epsilon': self.epsilon,\n",
        "            'gamma': self.gamma\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        if os.path.exists(path):\n",
        "            checkpoint = torch.load(path)\n",
        "            self.p_network.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            self.epsilon = checkpoint['epsilon']\n",
        "            self.gamma = checkpoint['gamma']\n",
        "        else:\n",
        "            print(f\"No saved model found at {path}\")\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        self.epsilon = max(self.ep_min, self.epsilon * self.ep_decay)\n"
      ],
      "metadata": {
        "id": "bLfetBb3lhco"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train"
      ],
      "metadata": {
        "id": "eArswsYwTXmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(num_episodes, epsilon=0.5, ep_decay=0.9, gamma=0.01, learning_rate=0.1):\n",
        "    agent1 = QLearningAgent(epsilon=epsilon, ep_decay=ep_decay, gamma=gamma, learning_rate=learning_rate)\n",
        "    agent2 = QLearningAgent(epsilon=epsilon, ep_decay=ep_decay, gamma=gamma, learning_rate=learning_rate)\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        boardEnv = C4Env()\n",
        "        board = boardEnv.board\n",
        "        game_over = False\n",
        "        turn = 0\n",
        "        total_turns = 0\n",
        "\n",
        "        while not game_over:\n",
        "            # Get Action\n",
        "            valid_actions = boardEnv.legal_actions()\n",
        "\n",
        "            if turn == 0:\n",
        "                action = agent1.get_action(board, valid_actions, boardEnv)\n",
        "                piece = RED\n",
        "            else:\n",
        "                action = agent2.get_action(board, valid_actions, boardEnv)\n",
        "                piece = YELLOW\n",
        "\n",
        "            # Take Action\n",
        "            if action == -1:\n",
        "                continue\n",
        "            boardEnv.step(action)\n",
        "\n",
        "            # Get Rewards\n",
        "            reward = 0\n",
        "            if boardEnv.winning_move(piece):\n",
        "                reward = 1 if turn == 0 else -1\n",
        "                game_over = True\n",
        "            elif np.all(boardEnv.legal_actions == False):\n",
        "                game_over = True\n",
        "\n",
        "            next_state = board.copy()\n",
        "\n",
        "            if turn == 0:\n",
        "                agent1.update(board, action, reward, next_state, game_over)\n",
        "            else:\n",
        "                agent2.update(board, action, -reward, next_state, game_over)\n",
        "\n",
        "            board = next_state\n",
        "            turn = 1 - turn\n",
        "            total_turns += 1\n",
        "\n",
        "        if episode % 1000 == 0:\n",
        "            print(f\"Episode {episode} completed: turns - {total_turns}\")\n",
        "            boardEnv.render()\n",
        "\n",
        "\n",
        "    return agent1, agent2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_N7pjVGmNET",
        "outputId": "efa4941e-53e0-4ffa-8073-d2af9cbc480d",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 completed: turns - 31\n",
            "Y.Y..YR\n",
            "R.R..RY\n",
            "R.R..YY\n",
            "YRY..RY\n",
            "RRRYRYR\n",
            "YRYRRYY\n",
            "\n",
            "Episode 1000 completed: turns - 9\n",
            ".......\n",
            "......R\n",
            "......R\n",
            "......R\n",
            "......R\n",
            "YYR..YY\n",
            "\n",
            "Episode 2000 completed: turns - 16\n",
            ".R.....\n",
            ".R.....\n",
            ".R...Y.\n",
            ".Y...Y.\n",
            ".R..YYY\n",
            "RRR.YYR\n",
            "\n",
            "Episode 3000 completed: turns - 16\n",
            ".......\n",
            ".......\n",
            "R.Y..Y.\n",
            "Y.Y..R.\n",
            "RRYR.Y.\n",
            "RYYR.R.\n",
            "\n",
            "Episode 4000 completed: turns - 15\n",
            "....R..\n",
            "....Y..\n",
            "..R.R..\n",
            "..RYY..\n",
            "..RYY..\n",
            "RYRYR..\n",
            "\n",
            "Episode 5000 completed: turns - 11\n",
            ".......\n",
            ".......\n",
            ".R.....\n",
            ".R....Y\n",
            ".RRR..Y\n",
            ".RYY..Y\n",
            "\n",
            "Episode 6000 completed: turns - 11\n",
            ".......\n",
            ".......\n",
            "..Y.R..\n",
            "..Y.R..\n",
            "..Y.R.R\n",
            "..RYR.Y\n",
            "\n",
            "Episode 7000 completed: turns - 17\n",
            "....Y..\n",
            "....R..\n",
            "R...Y..\n",
            "R..YR..\n",
            "RR.YY.Y\n",
            "RR.YR.Y\n",
            "\n",
            "Episode 8000 completed: turns - 13\n",
            "..R....\n",
            "..R....\n",
            "..R....\n",
            "Y.R....\n",
            "Y.YR...\n",
            "YYRY..R\n",
            "\n",
            "Episode 9000 completed: turns - 11\n",
            ".......\n",
            ".......\n",
            "..R....\n",
            "..RY...\n",
            "..RY..Y\n",
            "R.RY.YR\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manually train a new model\n",
        "\n",
        "agent1, agent2 = train(num_episodes)\n",
        "\n",
        "agent1.load(os.path.join(save_path, \"agent1.pth\"))\n",
        "agent2.load(os.path.join(save_path, \"agent2.pth\"))\n"
      ],
      "metadata": {
        "id": "xt8mwVsfyhtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gSMnQiB4zHf3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Fc8FtHkyr_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "-x0gNdtoTfwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Player v Computer"
      ],
      "metadata": {
        "id": "M7rnpvctTkKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to play a game using the trained model\n",
        "def play_game(turn=0): # turn = 0 -> player goes first; turn = 1 -> agent goes first\n",
        "    boardEnv = C4Env()\n",
        "    board = boardEnv.board\n",
        "    game_over = False\n",
        "    PLAYER = RED if turn==0 else YELLOW\n",
        "    AGENT = RED if turn==-1 else YELLOW\n",
        "    agent = agent2 if turn==0 else agent1\n",
        "\n",
        "    while not game_over:\n",
        "        valid_actions = boardEnv.legal_actions()\n",
        "\n",
        "        if turn == 0:\n",
        "            action = int(input(\"Enter Move: \"))\n",
        "            piece = PLAYER\n",
        "        else:\n",
        "            action = agent.get_action(board, valid_actions)\n",
        "            piece = AGENT\n",
        "\n",
        "        boardEnv.step(action)\n",
        "\n",
        "        if boardEnv.winning_move(piece):\n",
        "            boardEnv.render()\n",
        "            print(f\"{'Agent' if piece==AGENT else 'Player'} wins!\")\n",
        "            game_over = True\n",
        "        elif np.all(boardEnv.legal_actions == False):\n",
        "            boardEnv.render()\n",
        "            print(\"It's a draw!\")\n",
        "            game_over = True\n",
        "\n",
        "        boardEnv.render()\n",
        "        turn = 1 - turn\n",
        "\n",
        "# Play a game with the trained agents\n",
        "play_game(turn=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4YHrDL3r73j",
        "outputId": "b2e794fc-1060-436d-e926-9911ab56264b",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Move: 3\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "...R...\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".Y.R...\n",
            "\n",
            "Enter Move: 3\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "...R...\n",
            ".Y.R...\n",
            "\n",
            ".......\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "...R...\n",
            ".Y.R.Y.\n",
            "\n",
            "Enter Move: 3\n",
            ".......\n",
            ".......\n",
            ".......\n",
            "...R...\n",
            "...R...\n",
            ".Y.R.Y.\n",
            "\n",
            ".......\n",
            ".......\n",
            "...Y...\n",
            "...R...\n",
            "...R...\n",
            ".Y.R.Y.\n",
            "\n",
            "Enter Move: 1\n",
            ".......\n",
            ".......\n",
            "...Y...\n",
            "...R...\n",
            ".R.R...\n",
            ".Y.R.Y.\n",
            "\n",
            ".......\n",
            ".......\n",
            "...Y...\n",
            ".Y.R...\n",
            ".R.R...\n",
            ".Y.R.Y.\n",
            "\n",
            "Enter Move: 5\n",
            ".......\n",
            ".......\n",
            "...Y...\n",
            ".Y.R...\n",
            ".R.R.R.\n",
            ".Y.R.Y.\n",
            "\n",
            ".......\n",
            "...Y...\n",
            "...Y...\n",
            ".Y.R...\n",
            ".R.R.R.\n",
            ".Y.R.Y.\n",
            "\n",
            "Enter Move: 2\n",
            ".......\n",
            "...Y...\n",
            "...Y...\n",
            ".Y.R...\n",
            ".R.R.R.\n",
            ".YRR.Y.\n",
            "\n",
            ".......\n",
            "...Y...\n",
            ".Y.Y...\n",
            ".Y.R...\n",
            ".R.R.R.\n",
            ".YRR.Y.\n",
            "\n",
            "Enter Move: 1\n",
            ".......\n",
            ".R.Y...\n",
            ".Y.Y...\n",
            ".Y.R...\n",
            ".R.R.R.\n",
            ".YRR.Y.\n",
            "\n",
            ".......\n",
            ".R.Y...\n",
            ".Y.Y...\n",
            ".Y.R.Y.\n",
            ".R.R.R.\n",
            ".YRR.Y.\n",
            "\n",
            "Enter Move: 4\n",
            ".......\n",
            ".R.Y...\n",
            ".Y.Y...\n",
            ".Y.R.Y.\n",
            ".R.R.R.\n",
            ".YRRRY.\n",
            "\n",
            ".Y.....\n",
            ".R.Y...\n",
            ".Y.Y...\n",
            ".Y.R.Y.\n",
            ".R.R.R.\n",
            ".YRRRY.\n",
            "\n",
            "Enter Move: 4\n",
            ".Y.....\n",
            ".R.Y...\n",
            ".Y.Y...\n",
            ".Y.R.Y.\n",
            ".R.RRR.\n",
            ".YRRRY.\n",
            "\n",
            ".Y.Y...\n",
            ".R.Y...\n",
            ".Y.Y...\n",
            ".Y.R.Y.\n",
            ".R.RRR.\n",
            ".YRRRY.\n",
            "\n",
            "Enter Move: 2\n",
            ".Y.Y...\n",
            ".R.Y...\n",
            ".Y.Y...\n",
            ".Y.R.Y.\n",
            ".RRRRR.\n",
            ".YRRRY.\n",
            "\n",
            "Player wins!\n",
            ".Y.Y...\n",
            ".R.Y...\n",
            ".Y.Y...\n",
            ".Y.R.Y.\n",
            ".RRRRR.\n",
            ".YRRRY.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "agent1.save(os.path.join(save_path, \"agent1.pth\"))\n",
        "agent2.save(os.path.join(save_path, \"agent2.pth\"))"
      ],
      "metadata": {
        "id": "IxIMYy17xUq0",
        "outputId": "3356a2d1-8018-4ae9-92e7-4265fdd14ffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}